{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd6aadf-a1f6-4baf-aa15-c56e9a5f8d60",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights\n",
    "\n",
    "https://medium.com/@zergtant/use-weighted-loss-function-to-solve-imbalanced-data-classification-problems-749237f38b75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010baf86-e1b6-4965-8601-203724753d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af4ff73-06df-472c-b2a7-e9368ac93f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db88b55-7d7d-4ff6-93ef-14ec44df5f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 55)\n"
     ]
    }
   ],
   "source": [
    "filepath = 'covtype.data.csv'\n",
    "df = pd.read_csv(filepath, header=None)\n",
    "\n",
    "columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', \n",
    "           'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n",
    "           'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
    "           'Soil_Type1' ,'Soil_Type2' ,'Soil_Type3', 'Soil_Type4', 'Soil_Type5', \n",
    "           'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', \n",
    "           'Soil_Type11' ,'Soil_Type12' ,'Soil_Type13', 'Soil_Type14', 'Soil_Type15', \n",
    "           'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', \n",
    "           'Soil_Type21' ,'Soil_Type22' ,'Soil_Type23', 'Soil_Type24', 'Soil_Type25', \n",
    "           'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', \n",
    "           'Soil_Type31' ,'Soil_Type32' ,'Soil_Type33', 'Soil_Type34', 'Soil_Type35', \n",
    "           'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Cover_Type']\n",
    "target = 'Cover_Type'\n",
    "\n",
    "df.columns = columns\n",
    "df[target] = df[target] - 1 # recode the integer values\n",
    "df = df.astype(float) # convert all to float\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ad0a90-4201-4234-87de-77b75db97829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (366037, 54)\n",
      "Validation set: (40671, 54)\n",
      "Test set: (174304, 54)\n"
     ]
    }
   ],
   "source": [
    "# This split is according to original work\n",
    "'''\n",
    "df_list = []\n",
    "for c in range(0,len(df[target].unique())):\n",
    "    df_list.append(df.loc[df[target] == c]) # separate rows according to their classes\n",
    "\n",
    "random_state = 1\n",
    "n_train = 1620\n",
    "n_vald = 540\n",
    "#tmp = df_list[0].sample(n=n_train, random_state=0)\n",
    "#print(tmp)\n",
    "np_train = np.empty((0,len(columns)))\n",
    "np_vald = np.empty((0,len(columns)))\n",
    "np_test = np.empty((0,len(columns)))\n",
    "for df_ in df_list:\n",
    "    df_tr = df_.sample(n=n_train, random_state=random_state) # randomly get training instances\n",
    "    np_train = np.vstack((np_train, df_tr.values))\n",
    "    #np_train.append(df_tr.values)\n",
    "    df_ = df_.drop(index=df_tr.index) # drop training instances\n",
    "    \n",
    "    df_va = df_.sample(n=n_vald, random_state=random_state) # randomly get validation instances\n",
    "    np_vald = np.vstack((np_vald, df_va.values))\n",
    "    #np_vald.append(df_va.values)\n",
    "    df_ = df_.drop(index=df_va.index) # drop validation instances\n",
    "    \n",
    "    np_test = np.vstack((np_test, df_.values))\n",
    "    # np_test.append(df_) # the remaining is test instances   \n",
    "\n",
    "y_train = np_train[:,-1]\n",
    "y_vald = np_vald[:,-1]\n",
    "y_test = np_test[:,-1]\n",
    "X_train = np_train[:,:-1]\n",
    "X_vald = np_vald[:,:-1]\n",
    "X_test = np_test[:,:-1]\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[target].values.astype(np.float32)\n",
    "X = df.drop(columns=[target]).values\n",
    "\n",
    "# This split is according to Tab Survey (Borisov et al., 2022)\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, train_size=0.7, random_state=0)\n",
    "X_train, X_vald, y_train, y_vald = train_test_split(X_tmp, y_tmp, train_size=0.9, random_state=0)\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Validation set: {X_vald.shape}')\n",
    "print(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cfadff0-568d-4c4c-9f01-97cc4fe5f22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2., 3., 4., 5., 6.], dtype=float32), array([133505, 178299,  22654,   1745,   5973,  10941,  12920],\n",
      "      dtype=int64))\n",
      "(array([0., 1., 2., 3., 4., 5., 6.], dtype=float32), array([14837, 19804,  2519,   180,   670,  1197,  1464], dtype=int64))\n",
      "(array([0., 1., 2., 3., 4., 5., 6.], dtype=float32), array([63498, 85198, 10581,   822,  2850,  5229,  6126], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_vald, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b9d90-40f2-4dc9-9d6b-a2e6b92edff5",
   "metadata": {},
   "source": [
    "Dataset is imbalanced. Calculate the weights for each class. The weights will be used during training to make the model pay more attention to under-represented classes\n",
    "\n",
    "Let $w_c$ denotes the weight of class $c$. The weight is calculated as follows:\n",
    "\n",
    "$w_c = \\frac{1}{N_c} \\times \\frac{N}{C} = \\frac{N}{N_c \\times C}$\n",
    "\n",
    "where $N$ is the total number of instances, $N_c$ is the number of instances for class $c$ and $C$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e129a21-99e5-495b-a471-5ef5cc1dcf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_target_instances = np.bincount(y_train.astype(np.int64))\\nN = np.sum(num_target_instances)\\nC = len(num_target_instances)\\n\\nclass_weight = {}\\nfor c,s in enumerate(num_target_instances):\\n    w = (1 / s) * (N / C)\\n    class_weight[c] = w\\nprint(class_weight)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_target_instances = np.bincount(y_train.astype(np.int64))\n",
    "N = np.sum(num_target_instances)\n",
    "C = len(num_target_instances)\n",
    "\n",
    "class_weight = {}\n",
    "for c,s in enumerate(num_target_instances):\n",
    "    w = (1 / s) * (N / C)\n",
    "    class_weight[c] = w\n",
    "print(class_weight)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472e06a6-d0a1-4821-9af2-de8f7dc773ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_dataset(data, target, shuffle=True, batch_size=128):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((data, target))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(batch_size*2).batch(batch_size).prefetch(batch_size)\n",
    "    else:\n",
    "        ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 2048\n",
    "train_ds = array_to_dataset(X_train, y_train, batch_size=batch_size)\n",
    "vald_ds = array_to_dataset(X_vald, y_vald, shuffle=False, batch_size=batch_size)\n",
    "test_ds = array_to_dataset(X_test, y_test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3c4ba8f-528d-41e5-a951-08b0420cfba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32)\n",
      "(10, 32, 3)\n",
      "(10, 16, 3)\n",
      "(10, 16, 3)\n",
      "(10, 3, 32)\n",
      "(10, 16, 32)\n",
      "(10, 16, 32)\n",
      "input shape: (16, 32)\n",
      "score shape: (10, 16, 32, 32)\n",
      "score shape: (10, 16, 32)\n",
      "input shape: (10, 16, 32)\n",
      "(16, 10, 32)\n",
      "(16, 320)\n",
      "(16, 32)\n"
     ]
    }
   ],
   "source": [
    "## Multi-head/batch Attention\n",
    "'''\n",
    "b = 16\n",
    "d = 32\n",
    "inp = tf.random.uniform((b, d)) # (batch, n_features)\n",
    "print(inp.shape)\n",
    "\n",
    "c = 3\n",
    "h = 10\n",
    "w = tf.random.uniform((h, d, c)) # (head, n_features, class)\n",
    "print(w.shape)\n",
    "\n",
    "z = tf.matmul(inp, w) # (batch, n_features) dot (head, n_features, class) = (head, batch, class)\n",
    "print(z.shape)\n",
    "\n",
    "z = tf.nn.softmax(z, axis=-1)\n",
    "print(z.shape)\n",
    "\n",
    "r = 5.24\n",
    "w = tf.math.exp(w * r) # amplify weights; (head, n_features, class)\n",
    "w_ = tf.transpose(w, perm=(0, 2, 1))\n",
    "print(w_.shape) # (head, class, n_features)\n",
    "\n",
    "a = tf.matmul(z, w_) # (head, batch, class) dot (head, class, n_features) = (head, batch, n_features)\n",
    "print(a.shape) # (head, batch, n_features)\n",
    "#a = tf.reduce_mean(a, axis=1) # (head, n_features)\n",
    "print(a.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Multi-head IFE\n",
    "input_data = tf.random.normal((b, d))\n",
    "print(f'input shape: {input_data.shape}')\n",
    "input_scores = [tf.random.normal((h, b, d, 1)) for j in range(0,d)]\n",
    "input_scores = tf.concat(input_scores, axis=-1) # shape = (head, batch, n_features, n_features)\n",
    "print(f'score shape: {input_scores.shape}')\n",
    "#input_scores = tf.reduce_mean(input_scores, axis=[-1, 0]) # shape = (batch, n_features)\n",
    "#input_scores = tf.nn.softmax(input_scores, axis=-1) # shape = (batch, n_features)\n",
    "#print(f'score shape: {input_scores.shape}')\n",
    "\n",
    "# Instead of calculating mean across dimension -1 (n_features) and 0 (head), calculate mean across n_features only\n",
    "input_scores = tf.reduce_mean(input_scores, axis=[-1]) # shape = (head, batch, n_features)\n",
    "print(f'score shape: {input_scores.shape}')\n",
    "\n",
    "# input_data = tf.expand_dims(input_data, axis=0)\n",
    "input_data = tf.broadcast_to(input_data, [h, b, d])\n",
    "print(f'input shape: {input_data.shape}')\n",
    "\n",
    "weighted_inputs = input_data * input_scores\n",
    "#weighted_inputs = tf.reduce_mean(weighted_inputs, axis=[0])\n",
    "#print(f'weighted_inputs shape: {weighted_inputs.shape}')\n",
    "\n",
    "x = tf.transpose(weighted_inputs, perm=(1,0,2))\n",
    "print(x.shape)\n",
    "x = tf.reshape(x, shape=(b, -1))\n",
    "print(x.shape)\n",
    "x = tf.keras.layers.Dense(units=32)(x)\n",
    "print(x.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0a5a2b-ec3e-462e-bb04-7ed01155cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes: 7\n",
      "n_features: 54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2048, 7), dtype=float32, numpy=\n",
       "array([[0.05578536, 0.06902854, 0.09311974, ..., 0.20309812, 0.23704173,\n",
       "        0.24767171],\n",
       "       [0.14953183, 0.11989626, 0.1103613 , ..., 0.17568758, 0.12103122,\n",
       "        0.25354376],\n",
       "       [0.114999  , 0.08090202, 0.09831442, ..., 0.18825532, 0.2860886 ,\n",
       "        0.14139229],\n",
       "       ...,\n",
       "       [0.08044065, 0.10640021, 0.07542013, ..., 0.2559447 , 0.2230173 ,\n",
       "        0.18521957],\n",
       "       [0.07555072, 0.11363897, 0.11424036, ..., 0.14962895, 0.10252135,\n",
       "        0.37869936],\n",
       "       [0.17106344, 0.07528776, 0.13500437, ..., 0.1920265 , 0.10672691,\n",
       "        0.25328627]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IterativeFeatureExclusion as IFE\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "_, counts = np.unique(y_train, return_counts=True)\n",
    "n_classes = len(counts)\n",
    "ife_num_layers = 1\n",
    "clf_hidden_size = 65\n",
    "num_att = 8\n",
    "r = 5.9675\n",
    "\n",
    "print(f'n_classes: {n_classes}')\n",
    "print(f'n_features: {n_features}')\n",
    "\n",
    "ife_params = {'n_features': n_features,\n",
    "              'n_classes': n_classes,\n",
    "              'num_att': num_att,\n",
    "              'r': r,\n",
    "              'ife_num_layers': ife_num_layers, \n",
    "              'clf_hidden_size': clf_hidden_size,              \n",
    "             }\n",
    "model = IFE.IFENetClassifier(**ife_params)\n",
    "# model = model.build(input_shape=(n_features,))\n",
    "\n",
    "input_data = tf.random.normal((batch_size, n_features))\n",
    "model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b28db72b-0ac8-4f9e-8578-30b761a235ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport optuna\\nfrom tqdm.keras import TqdmCallback\\n\\ndef build_model(batch_size=None, hidden_units=None, drop_rate=None, input_shape=None, output_units=None):\\n#def build_model(ife_params=None):\\n    model = tf.keras.models.Sequential()\\n    model.add(tf.keras.layers.Dense(batch_size, input_shape=(input_shape,), activation=\\'relu\\'))\\n    model.add(tf.keras.layers.Dense(hidden_units, activation=\\'relu\\'))\\n    model.add(tf.keras.layers.Dropout(drop_rate))\\n    model.add(tf.keras.layers.Dense(output_units, activation=\\'softmax\\'))\\n    # model = IFE.IFENetClassifier(**ife_params)\\n    return model\\n\\n# utility function to create model trials\\ndef create_model(trial):\\n    # We optimize the numbers of layers, their units and learning rates \\n    #n_layers = trial.suggest_int(\"n_layers\", 1, 5)\\n    hidden_units = trial.suggest_int(\"hidden_size\", 64,68)\\n    #num_att = trial.suggest_int(\\'num_att\\', 8, 24)\\n    #r = trial.suggest_float(\\'drop_rate\\', 2.0, 7.0)\\n    #clf_hidden_size = trial.suggest_int(\\'clf_hidden_size\\', 64, 92)\\n    learning_rate = trial.suggest_float(\\'learning_rate\\', 1e-5, 0.02)\\n\\n    ife_params = {\\'n_features\\': n_features,\\n                  \\'n_classes\\': n_classes,\\n                  \\'num_att\\': num_att,\\n                  \\'r\\': r,\\n                  \\'ife_num_layers\\': ife_num_layers, \\n                  \\'clf_hidden_size\\': clf_hidden_size,              \\n                 }\\n\\n    model = build_model(batch_size=batch_size, hidden_units=hidden_units, drop_rate=drop_rate, input_shape=n_features, output_units=n_classes)\\n    #model = build_model(ife_params=ife_params)\\n    \\n    # compile the model\\n    model.compile(loss=\\'sparse_categorical_crossentropy\\',\\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\\n                  metrics=[\\'accuracy\\'])\\n    \\n    return model\\n\\n# Objective function\\ndef objective(trial):\\n    \\n    # instantiate model\\n    model_opt = create_model(trial)\\n    \\n    # fit the model\\n    model_opt.fit(train_ds, validation_data=vald_ds, epochs=epochs, verbose=0)\\n    \\n    # calculate accuracy score\\n    acc_score = model_opt.evaluate(test_ds, verbose=0)[1]\\n    \\n    return acc_score\\n\\nepochs = 10\\nn_trials = 25\\n# perform the optimization\\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"baseline model optimization\")\\nstudy.optimize(objective, n_trials=n_trials, n_jobs=6, show_progress_bar=True)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import optuna\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "def build_model(batch_size=None, hidden_units=None, drop_rate=None, input_shape=None, output_units=None):\n",
    "#def build_model(ife_params=None):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(batch_size, input_shape=(input_shape,), activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(hidden_units, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "    model.add(tf.keras.layers.Dense(output_units, activation='softmax'))\n",
    "    # model = IFE.IFENetClassifier(**ife_params)\n",
    "    return model\n",
    "\n",
    "# utility function to create model trials\n",
    "def create_model(trial):\n",
    "    # We optimize the numbers of layers, their units and learning rates \n",
    "    #n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    hidden_units = trial.suggest_int(\"hidden_size\", 64,68)\n",
    "    #num_att = trial.suggest_int('num_att', 8, 24)\n",
    "    #r = trial.suggest_float('drop_rate', 2.0, 7.0)\n",
    "    #clf_hidden_size = trial.suggest_int('clf_hidden_size', 64, 92)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 0.02)\n",
    "\n",
    "    ife_params = {'n_features': n_features,\n",
    "                  'n_classes': n_classes,\n",
    "                  'num_att': num_att,\n",
    "                  'r': r,\n",
    "                  'ife_num_layers': ife_num_layers, \n",
    "                  'clf_hidden_size': clf_hidden_size,              \n",
    "                 }\n",
    "\n",
    "    model = build_model(batch_size=batch_size, hidden_units=hidden_units, drop_rate=drop_rate, input_shape=n_features, output_units=n_classes)\n",
    "    #model = build_model(ife_params=ife_params)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Objective function\n",
    "def objective(trial):\n",
    "    \n",
    "    # instantiate model\n",
    "    model_opt = create_model(trial)\n",
    "    \n",
    "    # fit the model\n",
    "    model_opt.fit(train_ds, validation_data=vald_ds, epochs=epochs, verbose=0)\n",
    "    \n",
    "    # calculate accuracy score\n",
    "    acc_score = model_opt.evaluate(test_ds, verbose=0)[1]\n",
    "    \n",
    "    return acc_score\n",
    "\n",
    "epochs = 10\n",
    "n_trials = 25\n",
    "# perform the optimization\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"baseline model optimization\")\n",
    "study.optimize(objective, n_trials=n_trials, n_jobs=6, show_progress_bar=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252cb6d2-4b5b-4bdb-b155-d0a51b107401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tf.keras.Input(shape=(n_features,))\n",
    "#outputs = ifenet(inputs)\n",
    "#model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "565ff4db-873b-42a2-9c0d-811dab9ceda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "lr = 0.005\n",
    "lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, \n",
    "                                                              decay_steps=2000,\n",
    "                                                              decay_rate=0.95,\n",
    "                                                              staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "checkpoint_path = 'checkpoints/ifeNet_cover.h5'\n",
    "patience = 100\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=patience, monitor='val_loss'),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy')]\n",
    "\n",
    "epochs = 10\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5962958-ed12-440a-924f-cd9f26c7f7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "179/179 - 184s - loss: 0.6886 - accuracy: 0.7134 - val_loss: 2.0565 - val_accuracy: 0.2546 - 184s/epoch - 1s/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m saved_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_model/ifeNet_cover.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvald_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_path)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(saved_model_path)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saved_model_path = 'saved_model/ifeNet_cover.h5'\n",
    "model.fit(train_ds, validation_data=vald_ds, epochs=epochs, callbacks=callbacks, verbose=2)\n",
    "model.load_weights(checkpoint_path)\n",
    "model.save_weights(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd582be9-e439-4e15-b2cc-940a8c055dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.empty((0,))\n",
    "y_test = np.empty((0,))\n",
    "\n",
    "for data,label in test_ds:\n",
    "    y_hat = model(data)\n",
    "    y_hat = np.argmax(y_hat, axis=-1)\n",
    "    y_pred = np.append(y_pred, y_hat.ravel())\n",
    "\n",
    "    label = label.numpy()\n",
    "    y_test = np.append(y_test, label.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb55c4-d799-41dc-8aa1-f64661b402e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36560ec-89e1-4eaf-83eb-c06708341e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_scores = model.input_scores\n",
    "for col,score in zip(columns,feat_scores):\n",
    "    print(f'{col}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a07d6-961c-41c3-8abc-145653345ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
