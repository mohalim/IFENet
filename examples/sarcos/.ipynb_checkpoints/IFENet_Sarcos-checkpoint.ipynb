{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcd1f5d-0bf6-4595-bedd-dd3b26f9f9d1",
   "metadata": {},
   "source": [
    "Ref: \n",
    "\n",
    "http://gaussianprocess.org/gpml/data/\n",
    "\n",
    "Split ratio: https://github.com/Kaixhin/SARCOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d15824-1ec5-48f8-89b8-a58e8c075cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# sys.path.append(\"../tf_ifenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e6754d-b3a5-4c24-8455-2bc0404c9957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8025bc-89ba-4929-be7a-dc9c256e502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# from config import DataConfig, ModelConfig\n",
    "from tensorflow.keras.saving import register_keras_serializable, serialize_keras_object, deserialize_keras_object\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from typing import List, Optional\n",
    "\n",
    "def dataframe_to_dataset(dataframe, target_columns, batch_size=128, shuffle=True):\n",
    "    \"\"\"\n",
    "    Converts a Pandas DataFrame to a TensorFlow Dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe.\n",
    "        target_columns (list): List of column names to use as targets.\n",
    "        batch_size (int): Batch size for the dataset.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: A TensorFlow Dataset object.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        raise ValueError(\"Input must be a Pandas DataFrame.\")\n",
    "    if not isinstance(target_columns, list):\n",
    "        raise ValueError(\"Target columns must be provided as a list.\")\n",
    "    \n",
    "    try:\n",
    "        df_copy = dataframe.copy()\n",
    "        \n",
    "        for target in target_columns:\n",
    "            if target not in df_copy.columns:\n",
    "                raise KeyError(f\"Target column '{target}' not found in the DataFrame.\")\n",
    "            if df_copy[target].dtypes == 'object':\n",
    "                df_copy[target] = df_copy[target].astype('category').cat.codes\n",
    "        \n",
    "        # targets = df_copy.loc[:,target_columns]\n",
    "        targets = df_copy[target_columns].copy()\n",
    "        df_copy.drop(columns=target_columns, inplace=True)\n",
    "        \n",
    "        df_copy = {key: value.to_numpy()[:,tf.newaxis] for key, value in df_copy.items()}\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(df_copy), targets))\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(batch_size*2).batch(batch_size).prefetch(batch_size)\n",
    "        else:\n",
    "            dataset = dataset.batch(batch_size).prefetch(batch_size)\n",
    "        return dataset\n",
    "        \n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Target column '{e.args[0]}' is missing from the DataFrame.\")\n",
    "\n",
    "class DataConfig():\n",
    "    \"\"\"\n",
    "    Configuration for dataset preprocessing.\n",
    "    \n",
    "    Attributes:\n",
    "        categorical_column_names: List of categorical column names.\n",
    "        numerical_column_names: List of numerical column names.\n",
    "        category_output_mode: How categorical features are encoded ('one_hot', 'multi_hot', etc.).\n",
    "        is_normalization: Whether to normalize numerical features.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        categorical_column_names: List[str], \n",
    "        numerical_column_names: List[str],  \n",
    "        category_output_mode: str = 'one_hot', \n",
    "        is_normalization: bool = False\n",
    "    ):\n",
    "        if not isinstance(categorical_column_names, list) or not all(isinstance(col, str) for col in categorical_column_names):\n",
    "            raise TypeError(\"categorical_column_names must be a list of strings.\")\n",
    "        \n",
    "        if not isinstance(numerical_column_names, list) or not all(isinstance(col, str) for col in numerical_column_names):\n",
    "            raise TypeError(\"numerical_column_names must be a list of strings.\")\n",
    "        \n",
    "        if category_output_mode not in {'one_hot', 'multi_hot'}:\n",
    "            raise ValueError(\"category_output_mode must be 'one_hot' or 'multi_hot'\")\n",
    "\n",
    "        if not isinstance(is_normalization, bool):\n",
    "            raise TypeError(\"is_normalization must be a boolean value.\")\n",
    "        \n",
    "        self.categorical_column_names = categorical_column_names\n",
    "        self.numerical_column_names = numerical_column_names\n",
    "        self.category_output_mode = category_output_mode\n",
    "        self.is_normalization = is_normalization\n",
    "        \n",
    "    def get_config(self):\n",
    "        # Return the configuration parameters as a dictionary\n",
    "        config = {\n",
    "            \"categorical_column_names\": self.categorical_column_names,\n",
    "            \"numerical_column_names\": self.numerical_column_names,\n",
    "            \"category_output_mode\": self.category_output_mode,\n",
    "            \"is_normalization\": self.is_normalization\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(\n",
    "            config[\"categorical_column_names\"],\n",
    "            config[\"numerical_column_names\"],\n",
    "            config[\"category_output_mode\"],\n",
    "            config[\"is_normalization\"]\n",
    "        )\n",
    "            \n",
    "class ModelConfig():\n",
    "    \"\"\"\n",
    "    Configuration for model architecture.\n",
    "    \n",
    "    Attributes:\n",
    "        num_att: Number of attention heads.\n",
    "        r: An amplification coefficient. Must be 1 or greater.\n",
    "        clf_num_layers: Number of layers in the predictive layers. Must be 1 or greater.\n",
    "        clf_hidden_units: Hidden units in the classification head. \n",
    "                          Must align with clf_num_layers.\n",
    "        reduction_layer: Method for dimensionality reduction ('flatten', 'average').\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_att: int = 16, \n",
    "        r: float = 3.0, \n",
    "        clf_num_layers: int = 1, \n",
    "        clf_hidden_units: List[int] = [64], \n",
    "        reduction_layer: str = 'flatten'\n",
    "    ):\n",
    "        if not isinstance(num_att, int) or num_att <= 0:\n",
    "            raise ValueError(\"num_att must be a positive integer.\")\n",
    "\n",
    "        if not isinstance(r, (float, int)) or r < 1:\n",
    "            raise ValueError(\"r must be a float or integer greater than or equal to 1.\")\n",
    "\n",
    "        if not isinstance(clf_num_layers, int) or clf_num_layers < 1:\n",
    "            raise ValueError(\"clf_num_layers must be an integer greater than or equal to 1.\")\n",
    "            \n",
    "        if reduction_layer not in {'flatten', 'average'}:\n",
    "            raise ValueError(\"reduction_layer must be 'flatten' or 'average'\")\n",
    "\n",
    "        if not isinstance(clf_hidden_units, list) or not all(isinstance(unit, int) and unit > 0 for unit in clf_hidden_units):\n",
    "            raise TypeError(\"clf_hidden_units must be a list of positive integers.\")\n",
    "            \n",
    "        if len(clf_hidden_units) != clf_num_layers:\n",
    "            raise ValueError(\n",
    "                f\"clf_hidden_units must have exactly {clf_num_layers} elements. \"\n",
    "                f\"Got {len(clf_hidden_units)} elements instead.\"\n",
    "            )\n",
    "            \n",
    "        self.num_att = num_att\n",
    "        self.r = r\n",
    "        #self.ife_num_layers = 1\n",
    "        self.clf_num_layers = clf_num_layers\n",
    "        self.clf_hidden_units = clf_hidden_units\n",
    "        self.reduction_layer = reduction_layer\n",
    "\n",
    "    def get_config(self):\n",
    "        # Return the configuration parameters as a dictionary\n",
    "        config = {\n",
    "            \"num_att\": self.num_att,\n",
    "            \"r\": self.r,\n",
    "            #\"ife_num_layers\": self.ife_num_layers,\n",
    "            \"clf_num_layers\": self.clf_num_layers,\n",
    "            \"clf_hidden_units\": self.clf_hidden_units,\n",
    "            \"reduction_layer\": self.reduction_layer\n",
    "        }\n",
    "        return config\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(\n",
    "            config[\"num_att\"],\n",
    "            config[\"r\"],\n",
    "            config[\"clf_num_layers\"],\n",
    "            config[\"clf_hidden_units\"],\n",
    "            config[\"reduction_layer\"]\n",
    "        )\n",
    "\n",
    "@register_keras_serializable(name=\"_Attention\")\n",
    "class _Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, attn_norm_fn, num_att, r=2, initializer=\"glorot_uniform\", name=None, **kwargs):\n",
    "        super(_Attention, self).__init__()\n",
    "        self.units = units # number of classes/responses\n",
    "        self.attn_norm_fn = attn_norm_fn\n",
    "        self.num_att = num_att\n",
    "        self.r = r\n",
    "        self.initializer = initializer\n",
    "        if self.attn_norm_fn == 'sigmoid':\n",
    "            self.norm_function = tf.keras.layers.Activation(activation='sigmoid')\n",
    "        else:\n",
    "            self.norm_function = tf.keras.layers.Softmax()\n",
    "\n",
    "    def build(self, input_shape): # input_shape = (batch, n_features)\n",
    "        self.kernel = self.add_weight(shape=(self.num_att, input_shape[-1], self.units),\n",
    "                                      initializer=self.initializer,\n",
    "                                      trainable=True,\n",
    "                                      name='kernel') # shape = (num_att, n_features, n_outputs)\n",
    "\n",
    "    def call(self, inputs): # input_shape = (batch, n_features)\n",
    "        z = tf.matmul(inputs, self.kernel) # (batch, n_features) dot (num_att, n_features, n_outputs) = (num_att, batch, n_outputs)\n",
    "        # z = tf.nn.softmax(z, axis=-1) # (num_att, batch, n_outputs)\n",
    "        z = self.norm_function(z) # (num_att, batch, n_outputs)\n",
    "        \n",
    "        w = tf.math.exp(self.kernel * self.r) # amplify weights\n",
    "        outputs = tf.matmul(z, tf.transpose(w, perm=(0,2,1)))  # (num_att, batch, n_outputs) dot (num_att, n_outputs, n_features) = (num_att, batch, n_features)\n",
    "        # outputs = tf.reduce_mean(a, axis=[1])  # shape = (batch, n_features)\n",
    "        return outputs # (num_att, batch, n_features)\n",
    "\n",
    "    def get_config(self):\n",
    "        # Return the configuration parameters as a dictionary\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "            \"units\": self.units,\n",
    "            \"attn_norm_fn\": self.attn_norm_fn,\n",
    "            \"num_att\": self.num_att,\n",
    "            \"r\": self.r,\n",
    "            \"initializer\": self.initializer\n",
    "            })\n",
    "        return config\n",
    "     \n",
    "    #@classmethod\n",
    "    #def from_config(cls, config):\n",
    "    #    return cls(**config)\n",
    "\n",
    "@register_keras_serializable(name=\"_IterativeFeatureExclusion\")\n",
    "class _IterativeFeatureExclusion(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_features, n_outputs, attn_norm_fn, num_att=8, r=2, name=None, **kwargs):\n",
    "        super(_IterativeFeatureExclusion, self).__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_outputs = n_outputs\n",
    "        self.attn_norm_fn = attn_norm_fn\n",
    "        self.num_att = num_att\n",
    "        self.r = r\n",
    "        \n",
    "        self.attentions = [_Attention(self.n_outputs, self.attn_norm_fn, self.num_att, self.r) for i in range(self.n_features)]\n",
    "        mask_ones = np.ones((n_features,), dtype=np.int8)\n",
    "        self.masks = []\n",
    "        for j in range(0,n_features):\n",
    "            mask = mask_ones.copy()\n",
    "            mask[j] = 0\n",
    "            self.masks.append(tf.constant(mask, dtype=tf.float32))\n",
    "        #self.masks = tf.stack(self.masks, axis=1)\n",
    "\n",
    "    def call(self, inputs):       # input shape = (batch, n_features)\n",
    "        input_scores = []\n",
    "        for mask, attention in zip(self.masks,self.attentions):\n",
    "            inputs_masked = inputs * mask # shape = (num_att, batch, n_features)\n",
    "            z = tf.expand_dims(attention(inputs_masked), axis=-1) # (num_att, batch, n_features, 1)\n",
    "            input_scores.append(z)\n",
    "            \n",
    "        input_scores = tf.concat(input_scores, axis=-1) # shape = (num_att, batch, n_features, n_features)\n",
    "        input_scores = tf.reduce_mean(input_scores, axis=[-1]) # shape = (num_att, batch, n_features)\n",
    "        input_scores = tf.nn.softmax(input_scores, axis=-1) # shape = (num_att, batch, n_features)\n",
    "        return input_scores\n",
    "\n",
    "    def get_config(self):\n",
    "        # Serialize the list of attention layers using serialize_keras_object\n",
    "        attention_configs = [serialize_keras_object(attn) for attn in self.attentions]\n",
    "\n",
    "        # Convert masks into a list of arrays\n",
    "        masks = [mask.numpy() for mask in self.masks]\n",
    "\n",
    "        # Return a configuration dictionary including parameters and serialized layers\n",
    "        base_config = super(_IterativeFeatureExclusion, self).get_config()\n",
    "        config = {\n",
    "            **base_config,\n",
    "            \"n_features\": self.n_features,\n",
    "            \"n_outputs\": self.n_outputs,\n",
    "            \"attn_norm_fn\": self.attn_norm_fn,\n",
    "            \"num_att\": self.num_att,\n",
    "            \"r\": self.r,\n",
    "            \"attentions\": attention_configs,  # serialized attention layers\n",
    "            #\"masks\": masks  # serialized masks\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Reconstruct the attention layers from the serialized configurations\n",
    "        attentions = [deserialize_keras_object(attn_config) for attn_config in config[\"attentions\"]]\n",
    "\n",
    "        # Reconstruct masks\n",
    "        #masks = [tf.constant(mask, dtype=tf.float32) for mask in config[\"masks\"]]\n",
    "\n",
    "        # Reconstruct the layer\n",
    "        layer = cls(\n",
    "            n_features=config[\"n_features\"],\n",
    "            n_outputs=config[\"n_outputs\"],\n",
    "            attn_norm_fn=config[\"attn_norm_fn\"],\n",
    "            num_att=config[\"num_att\"],\n",
    "            r=config[\"r\"]\n",
    "        )\n",
    "        # Assign the reconstructed attentions and masks to the layer\n",
    "        layer.attentions = attentions\n",
    "        #layer.masks = masks\n",
    "        return layer\n",
    "\n",
    "@register_keras_serializable(name=\"_IFEModule\")\n",
    "class _IFEModule(tf.keras.Model):\n",
    "    def __init__(self, data_config, model_config):\n",
    "        super(_IFEModule, self).__init__()\n",
    "        self._attn_norm_fn = 'softmax'\n",
    "\n",
    "        self._data_config = data_config\n",
    "        self._model_config = model_config\n",
    "\n",
    "        self._categorical_column_names = self._data_config.categorical_column_names\n",
    "        self._numerical_column_names = self._data_config.numerical_column_names\n",
    "        self._category_output_mode = self._data_config.category_output_mode\n",
    "        self._is_normalization = self._data_config.is_normalization\n",
    "        \n",
    "        self._num_att = self._model_config.num_att\n",
    "        self._r = self._model_config.r\n",
    "        # self._ife_num_layers = model_config.ife_num_layers\n",
    "\n",
    "        self._n_features = 0\n",
    "        self._encoder_layers = {}\n",
    "        \n",
    "        self.feature_indices = {}\n",
    "        self.input_scores = None\n",
    "\n",
    "    def _get_category_encoding_layer(self, name, dataset, dtype, max_tokens=None):\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        if dtype == tf.string:\n",
    "            index = tf.keras.layers.StringLookup(max_tokens=max_tokens)\n",
    "        elif dtype == tf.int64:\n",
    "            index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)\n",
    "        \n",
    "        index.adapt(feature_ds)\n",
    "        encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size(), output_mode=self._category_output_mode, name=name)\n",
    "        return lambda feature: encoder(index(feature))\n",
    "    \n",
    "    def _get_numerical_encoding_layer(self, name, dataset):\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        \n",
    "        if self._is_normalization:\n",
    "            encoder = tf.keras.layers.Normalization(axis=None)\n",
    "            encoder.adapt(feature_ds)\n",
    "            return lambda feature: encoder(feature)\n",
    "            return encoder\n",
    "        else:\n",
    "            return lambda feature: tf.cast(feature, dtype=tf.float32)\n",
    "        \n",
    "    def _create_encoder_layers(self, dataset, feature_names, feature_dtypes):\n",
    "        for name in feature_names:\n",
    "            if name in self._categorical_column_names:\n",
    "                layer = Lambda(self._get_category_encoding_layer(name, dataset, feature_dtypes[name]))\n",
    "                self._encoder_layers[name] = layer\n",
    "            elif name in self._numerical_column_names:\n",
    "                layer = Lambda(self._get_numerical_encoding_layer(name, dataset))\n",
    "                self._encoder_layers[name] = layer\n",
    "\n",
    "        st = 0\n",
    "        ed = 0\n",
    "        n_features = 0\n",
    "        for name, layer in self._encoder_layers.items():\n",
    "            example_input = next(iter(dataset.map(lambda x, y: x[name]))).numpy()\n",
    "            example_output = layer(example_input)\n",
    "            feature_size = example_output.shape[-1]  # Store the size (last dimension)\n",
    "            ed = st + feature_size\n",
    "            n_features = ed \n",
    "            index = list([st, ed])\n",
    "            st = ed\n",
    "            self.feature_indices[name] = index\n",
    "\n",
    "        return n_features\n",
    "\n",
    "    def get_feature_importance(self, columns):\n",
    "        feat_scores = np.mean(self.input_scores, axis=(0,1))\n",
    "\n",
    "        feat_rank = {}\n",
    "        for col,score in zip(columns, feat_scores):\n",
    "            feat_rank[col] = score\n",
    "        \n",
    "        df_feat_rank = pd.DataFrame(list(feat_rank.items()), columns=['Feature', 'Score'])\n",
    "        df_feat_rank.sort_values(by='Score', ascending=False)\n",
    "        return df_feat_rank\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(_IFEModule, self).get_config()\n",
    "        \n",
    "        # Serialize the data_config and model_config\n",
    "        data_config_dict = self._data_config.get_config()\n",
    "        model_config_dict = self._model_config.get_config()\n",
    "\n",
    "        # Serialize the encoder layers (which are created dynamically)\n",
    "        encoder_layers_config = {name: serialize_keras_object(layer) for name, layer in self._encoder_layers.items()}\n",
    "\n",
    "        # Return the complete configuration\n",
    "        config = {\n",
    "            **base_config,\n",
    "            \"data_config\": data_config_dict,\n",
    "            \"model_config\": model_config_dict,\n",
    "            \"encoder_layers\": encoder_layers_config,\n",
    "        }\n",
    "        \n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Deserialize the DataConfig and ModelConfig\n",
    "        data_config = DataConfig.from_config(config['data_config'])\n",
    "        model_config = ModelConfig.from_config(config['model_config'])\n",
    "\n",
    "        # Create an instance of _IFEModule\n",
    "        instance = cls(data_config, model_config)\n",
    "        \n",
    "        # Deserialize the encoder layers and assign them to the model\n",
    "        encoder_layers = {name: deserialize_keras_object(layer_config) for name, layer_config in config['encoder_layers'].items()}\n",
    "        instance._encoder_layers = encoder_layers\n",
    "        \n",
    "        # Return the reconstructed model\n",
    "        return instance\n",
    "    \n",
    "@register_keras_serializable(name=\"IFENetRegressor\")\n",
    "class IFENetRegressor(_IFEModule):\n",
    "    def __init__(self, data_config, model_config):\n",
    "        super(IFENetRegressor, self).__init__(data_config, model_config)\n",
    "\n",
    "        self.target_activation='linear'\n",
    "        # self._data_config = data_config\n",
    "        self._model_config = model_config\n",
    "\n",
    "        self._clf_num_layers = self._model_config.clf_num_layers\n",
    "        self._clf_hidden_units = self._model_config.clf_hidden_units\n",
    "        self._reduction = self._model_config.reduction_layer\n",
    "\n",
    "    def build_model(self, dataset):\n",
    "        if not isinstance(dataset, tf.data.Dataset):\n",
    "            raise ValueError(f\"Input must be a tf.data.Dataset, got {type(dataset)}.\")\n",
    "\n",
    "        feature_dtypes = {key: spec.dtype for key, spec in dataset.element_spec[0].items()}\n",
    "        feature_names = list(feature_dtypes.keys())\n",
    "        \n",
    "        self._n_features = self._create_encoder_layers(dataset, feature_names, feature_dtypes)\n",
    "\n",
    "        self._preprocess = tf.keras.layers.BatchNormalization(name='preprocess_batch_norm')\n",
    "\n",
    "        # Determine the number of responses\n",
    "        targets = next(iter(dataset.map(lambda x,y: y))).numpy()\n",
    "        n_outputs = targets.shape[1]\n",
    "        \n",
    "        self._ife_attn = _IterativeFeatureExclusion(self._n_features, n_outputs, self._attn_norm_fn, self._num_att, self._r)\n",
    "\n",
    "        # Build the predictive layers\n",
    "        clf_hidden_layers = []\n",
    "        for l in range(0, self._clf_num_layers):\n",
    "            clf_hidden_layers.append(tf.keras.layers.Dense(units=self._clf_hidden_units[l], activation='relu'))\n",
    "            clf_hidden_layers.append(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        if self._reduction == 'flatten':\n",
    "            self._reduction_layer = tf.keras.layers.Flatten()\n",
    "        elif self._reduction == 'average':\n",
    "            self._reduction_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        \n",
    "        self.clf_hidden_layers = tf.keras.Sequential(clf_hidden_layers, name='fc_hidden_layers')\n",
    "        self.fc_out = tf.keras.layers.Dense(units=n_outputs, activation=self.target_activation, name='fc_out')\n",
    "        \n",
    "    def call(self, inputs): # (batch, n_features)\n",
    "        # preprocessing the inputs\n",
    "        features = [self._encoder_layers[name](inputs[name]) for name in self._encoder_layers]\n",
    "        \n",
    "        features = tf.concat(features, axis=1)\n",
    "\n",
    "        # features are the preprocessed inputs\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        x = self._preprocess(features) # (batch, n_features)\n",
    "        norm_inputs = x\n",
    "        norm_inputs = tf.broadcast_to(norm_inputs, [self._num_att, batch_size, self._n_features]) # expand and broadcast it to the shape of input_scores\n",
    "        \n",
    "        self.input_scores = self._ife_attn(x)\n",
    "        x = norm_inputs * self.input_scores # (head, batch, n_features)\n",
    "\n",
    "        x = tf.transpose(x, perm=(1,0,2)) # (batch, head, n_features)\n",
    "        x = self._reduction_layer(x)\n",
    "\n",
    "        x = self.clf_hidden_layers(x)\n",
    "        outputs = self.fc_out(x)\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        # Serialize configuration of parent class (_IFEModule)\n",
    "        base_config = super(IFENetRegressor, self).get_config()\n",
    "\n",
    "        # Serialize the layer configurations for the layers created in build_model\n",
    "        preprocess_config = self._preprocess.get_config()  \n",
    "        ife_attn_config = self._ife_attn.get_config()  \n",
    "        reduction_config = self._reduction_layer.get_config()\n",
    "        # clf_hidden_layers_config = [layer.get_config() for layer in self.clf_hidden_layers.layers]\n",
    "        clf_hidden_layers_config = self.clf_hidden_layers.get_config()\n",
    "        fc_out_config = self.fc_out.get_config()\n",
    "\n",
    "        # Serialize the encoder layers (which are created dynamically)\n",
    "        encoder_layers_config = {name: serialize_keras_object(layer) for name, layer in self._encoder_layers.items()}\n",
    "        \n",
    "        config = {\n",
    "            **base_config,\n",
    "            \"n_features\": self._n_features,\n",
    "            \"target_activation\": self.target_activation,\n",
    "            \"clf_num_layers\": self._clf_num_layers,\n",
    "            \"clf_hidden_units\": self._clf_hidden_units,\n",
    "            \"reduction\": self._reduction,\n",
    "            \"reduction_layer\": reduction_config,\n",
    "            \"preprocess_config\": preprocess_config,\n",
    "            \"ife_attn_config\": ife_attn_config,\n",
    "            \"clf_hidden_layers_config\": clf_hidden_layers_config,\n",
    "            \"fc_out_config\": fc_out_config,\n",
    "            \"encoder_layers\": encoder_layers_config,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Restore the base configuration from the parent class (_IFEModule)\n",
    "        data_config = DataConfig.from_config(config['data_config'])\n",
    "        model_config = ModelConfig.from_config(config['model_config'])\n",
    "        \n",
    "        # Create an instance of IFENetRegressor with the restored configurations\n",
    "        instance = cls(data_config, model_config)\n",
    "\n",
    "        # Set the custom configurations for IFENetRegressor\n",
    "        instance.target_activation = config[\"target_activation\"]\n",
    "        instance._clf_num_layers = config[\"clf_num_layers\"]\n",
    "        instance._clf_hidden_units = config[\"clf_hidden_units\"]\n",
    "        instance._reduction = config[\"reduction\"]\n",
    "        instance._n_features = config[\"n_features\"]\n",
    "    \n",
    "        # Deserialize and set layers\n",
    "        instance._preprocess = tf.keras.layers.BatchNormalization.from_config(config[\"preprocess_config\"])\n",
    "        instance._ife_attn = _IterativeFeatureExclusion.from_config(config[\"ife_attn_config\"])\n",
    "        instance._reduction_layer = tf.keras\n",
    "        instance.clf_hidden_layers = tf.keras.Sequential.from_config(config[\"clf_hidden_layers_config\"])\n",
    "        instance.fc_out = tf.keras.layers.Dense.from_config(config[\"fc_out_config\"])\n",
    "\n",
    "        # Deserialize the encoder layers and assign them to the model\n",
    "        encoder_layers = {name: deserialize_keras_object(layer_config) for name, layer_config in config[\"encoder_layers\"].items()}\n",
    "        instance._encoder_layers = encoder_layers\n",
    "    \n",
    "        # Rebuild the model layers (you need to pass a dataset to reconstruct model layers)\n",
    "        # Example: Replace 'dummy_dataset' with actual dataset\n",
    "        # instance.build_model(dummy_dataset)  # dummy_dataset should be passed to rebuild layers\n",
    "    \n",
    "        return instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a5f3ea-be07-4747-ac52-bba41e8aad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train = 'sarcos_inv.csv'\n",
    "filepath_test = 'sarcos_inv_test.csv'\n",
    "\n",
    "num_col_names = [str(c) for c in range(0,21)]\n",
    "cat_col_names = []\n",
    "target_columns = [str(c) for c in range(21,28)]\n",
    "\n",
    "# read training set\n",
    "train = pd.read_csv(filepath_train)\n",
    "\n",
    "# read test set\n",
    "test = pd.read_csv(filepath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2057f03-5618-4211-a734-f3710e90235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (39984, 28)\n",
      "Validation set: (4500, 28)\n",
      "Test set: (4449, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, vald = train_test_split(train, test_size=4500, random_state=0)\n",
    "\n",
    "print(f'Training set: {train.shape}')\n",
    "print(f'Validation set: {vald.shape}')\n",
    "print(f'Test set: {test.shape}')\n",
    "\n",
    "batch_size = 2048\n",
    "train_ds = dataframe_to_dataset(train, target_columns, batch_size=batch_size)\n",
    "vald_ds = dataframe_to_dataset(vald, target_columns, shuffle=False, batch_size=batch_size)\n",
    "test_ds = dataframe_to_dataset(test, target_columns, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2cf770-717d-470c-ae6a-ca28ab7d5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(categorical_column_names=cat_col_names, \n",
    "                         numerical_column_names=num_col_names,\n",
    "                         category_output_mode='one_hot',\n",
    "                         is_normalization=False)\n",
    "model_config = ModelConfig(num_att=16,\n",
    "                           r=3.5,\n",
    "                           clf_num_layers=1,\n",
    "                           clf_hidden_units=[32],\n",
    "                           reduction_layer='flatten')\n",
    "\n",
    "model = IFENetRegressor(data_config, model_config)\n",
    "model.build_model(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880a1ff3-f3b8-40d6-9acb-ae5a230d5fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "lr = 0.015\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "checkpoint_path = 'checkpoints/ifeNet_sarcos.h5'\n",
    "patience = 20\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=patience, monitor='val_loss'),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_loss')]\n",
    "\n",
    "epochs = 2\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d115a606-fe04-4b8a-8f68-a1037f1a69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "20/20 - 12s - loss: 288.8676 - mse: 288.8676 - val_loss: 249.2207 - val_mse: 249.2207 - 12s/epoch - 618ms/step\n",
      "Epoch 2/2\n",
      "20/20 - 6s - loss: 128.9329 - mse: 128.9329 - val_loss: 105.2535 - val_mse: 105.2535 - 6s/epoch - 317ms/step\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = 'saved_model/ifeNet_sarcos.keras'\n",
    "#saved_weights_path = 'saved_model/ifeNet_sarcos.h5'\n",
    "model.fit(train_ds, validation_data=vald_ds, epochs=epochs, callbacks=callbacks, verbose=2)\n",
    "#model.load_weights(checkpoint_path)\n",
    "#model.save_weights(saved_weights_path)\n",
    "model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d149952e-635a-4f73-937f-2cd65400d8bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IFENetRegressor' object has no attribute '_reduction_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m saved_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_model/ifeNet_sarcos.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ifenet \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         )\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 246\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:731\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    729\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build_config:\n\u001b[1;32m--> 731\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    732\u001b[0m compile_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_config:\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:2331\u001b[0m, in \u001b[0;36mLayer.build_from_config\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   2329\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2331\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\engine\\training.py:541\u001b[0m, in \u001b[0;36mModel.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can only call `build()` on a model if its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`call()` method accepts an `inputs` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot build your model by calling `build` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif your layers do not support float type inputs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`call` is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[3], line 473\u001b[0m, in \u001b[0;36mIFENetRegressor.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    470\u001b[0m x \u001b[38;5;241m=\u001b[39m norm_inputs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_scores \u001b[38;5;66;03m# (head, batch, n_features)\u001b[39;00m\n\u001b[0;32m    472\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(x, perm\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;66;03m# (batch, head, n_features)\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduction_layer\u001b[49m(x)\n\u001b[0;32m    475\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclf_hidden_layers(x)\n\u001b[0;32m    476\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(x)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'IFENetRegressor' object has no attribute '_reduction_layer'"
     ]
    }
   ],
   "source": [
    "saved_model_path = 'saved_model/ifeNet_sarcos.keras'\n",
    "ifenet = tf.keras.models.load_model(saved_model_path, safe_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b2b282-7d61-40ac-970a-daa7622fff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.empty((0,len(target_columns)))\n",
    "y_test = np.empty((0,len(target_columns)))\n",
    "\n",
    "for data,label in test_ds:\n",
    "    y_hat = model(data)\n",
    "    y_pred = np.append(y_pred, y_hat, axis=0)\n",
    "    \n",
    "    label = label.numpy()\n",
    "    y_test = np.append(y_test, label, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50cf3817-245d-4c86-8f77-48a4c181c08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.48524393179493547\n",
      "MSE: 67.59077267535356\n",
      "MAE: 5.576514489798536\n"
     ]
    }
   ],
   "source": [
    "print(f'R2 Score: {r2_score(y_test, y_pred)}')\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d721e2-bcbd-4780-a494-4d7373355979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_weights_path = 'saved_model/ifeNet_sarcos.h5'\n",
    "#model.load_weights(saved_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "59420d56-a0c6-4187-9aff-562a8d4f356f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Requested the deserialization of a Lambda layer with a Python `lambda` inside it. This carries a potential risk of arbitrary code execution and thus it is disallowed by default. If you trust the source of the saved model, you can pass `safe_mode=False` to the loading function in order to allow Lambda layer loading.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m saved_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_model/ifeNet_sarcos.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ifenet \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         )\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 246\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
      "Cell \u001b[1;32mIn[73], line 529\u001b[0m, in \u001b[0;36mIFENetRegressor.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    526\u001b[0m instance\u001b[38;5;241m.\u001b[39mfc_out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense\u001b[38;5;241m.\u001b[39mfrom_config(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc_out_config\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# Deserialize the encoder layers and assign them to the model\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m encoder_layers \u001b[38;5;241m=\u001b[39m {name: deserialize_keras_object(layer_config) \u001b[38;5;28;01mfor\u001b[39;00m name, layer_config \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    530\u001b[0m instance\u001b[38;5;241m.\u001b[39m_encoder_layers \u001b[38;5;241m=\u001b[39m encoder_layers\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Rebuild the model layers (you need to pass a dataset to reconstruct model layers)\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Example: Replace 'dummy_dataset' with actual dataset\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[73], line 529\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    526\u001b[0m instance\u001b[38;5;241m.\u001b[39mfc_out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense\u001b[38;5;241m.\u001b[39mfrom_config(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc_out_config\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# Deserialize the encoder layers and assign them to the model\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m encoder_layers \u001b[38;5;241m=\u001b[39m {name: \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_config\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, layer_config \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    530\u001b[0m instance\u001b[38;5;241m.\u001b[39m_encoder_layers \u001b[38;5;241m=\u001b[39m encoder_layers\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Rebuild the model layers (you need to pass a dataset to reconstruct model layers)\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Example: Replace 'dummy_dataset' with actual dataset\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\layers\\core\\lambda_layer.py:327\u001b[0m, in \u001b[0;36mLambda.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config, custom_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    326\u001b[0m     config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 327\u001b[0m     function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_function_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodule\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m     output_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_function_from_config(\n\u001b[0;32m    332\u001b[0m         config,\n\u001b[0;32m    333\u001b[0m         custom_objects,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    337\u001b[0m     )\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\tf2_14\\lib\\site-packages\\keras\\src\\layers\\core\\lambda_layer.py:394\u001b[0m, in \u001b[0;36mLambda._parse_function_from_config\u001b[1;34m(cls, config, custom_objects, func_attr_name, module_attr_name, func_type_attr_name)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m function_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m serialization_lib\u001b[38;5;241m.\u001b[39min_safe_mode():\n\u001b[1;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested the deserialization of a Lambda layer with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython `lambda` inside it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    397\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis carries a potential risk of arbitrary code execution \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus it is disallowed by default. If you trust the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource of the saved model, you can pass `safe_mode=False` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the loading function in order to allow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLambda layer loading.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         )\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# /!\\ Unsafe deserialization from bytecode! Danger! /!\\\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     function \u001b[38;5;241m=\u001b[39m generic_utils\u001b[38;5;241m.\u001b[39mfunc_load(\n\u001b[0;32m    405\u001b[0m         config[func_attr_name], globs\u001b[38;5;241m=\u001b[39mglobs\n\u001b[0;32m    406\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Requested the deserialization of a Lambda layer with a Python `lambda` inside it. This carries a potential risk of arbitrary code execution and thus it is disallowed by default. If you trust the source of the saved model, you can pass `safe_mode=False` to the loading function in order to allow Lambda layer loading."
     ]
    }
   ],
   "source": [
    "saved_model_path = 'saved_model/ifeNet_sarcos.keras'\n",
    "ifenet = tf.keras.models.load_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7735b1d-ed3f-41c0-a0f3-12337fb51307",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_feature_importance() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_feature_importance() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "model.get_feature_importance(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a4f9710-8730-468f-9ad3-3ec88bccc787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_response: 7\n",
      "n_features: 21\n"
     ]
    }
   ],
   "source": [
    "from ife import IFENetRegressor\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "_, counts = np.unique(y_train, return_counts=True)\n",
    "n_response = len(targets)\n",
    "ife_num_layers = 1\n",
    "clf_num_layers = 1\n",
    "clf_hidden_units = [128]\n",
    "reduction_layer = 'flatten'\n",
    "num_att = 128\n",
    "r = 4.0\n",
    "\n",
    "print(f'n_response: {n_response}')\n",
    "print(f'n_features: {n_features}')\n",
    "\n",
    "ife_params = {'n_features': n_features,\n",
    "              'n_outputs': n_response,\n",
    "              'num_att': num_att,\n",
    "              'r': r,\n",
    "              'ife_num_layers': ife_num_layers, \n",
    "              'clf_num_layers': clf_num_layers,\n",
    "              'clf_hidden_units': clf_hidden_units,\n",
    "              'reduction_layer': reduction_layer\n",
    "             }\n",
    "model = IFENetRegressor(**ife_params)\n",
    "model.build(input_shape=(None,n_features))\n",
    "\n",
    "path_saved_model = 'saved_model/ifeNet_sarcos_att_128.h5'\n",
    "model.load_weights(path_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd8ff50-e8d9-4eda-81c9-b9604065355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.192080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.096904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.077443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.071178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.063495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.063181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.049514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.048306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.038565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.032125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.030934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.028309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.026951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.026544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.026267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.023664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.023350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.022786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature     Score\n",
       "14       14  0.192080\n",
       "0         0  0.096904\n",
       "17       17  0.077443\n",
       "1         1  0.071178\n",
       "2         2  0.063495\n",
       "15       15  0.063181\n",
       "7         7  0.049514\n",
       "16       16  0.048306\n",
       "3         3  0.038565\n",
       "12       12  0.032125\n",
       "4         4  0.030934\n",
       "19       19  0.028400\n",
       "9         9  0.028309\n",
       "13       13  0.026951\n",
       "10       10  0.026544\n",
       "11       11  0.026267\n",
       "8         8  0.023664\n",
       "18       18  0.023350\n",
       "5         5  0.022786\n",
       "20       20  0.015400\n",
       "6         6  0.014600"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_scores = model.input_scores\n",
    "feat_scores = np.mean(feat_scores, axis=(0,1))\n",
    "\n",
    "feat_rank = {}\n",
    "for col,score in zip(features,feat_scores):\n",
    "    #print(f'{col}: {score}')\n",
    "    feat_rank[col] = score\n",
    "\n",
    "df_feat_rank = pd.DataFrame(list(feat_rank.items()), columns=['Feature', 'Score'])\n",
    "df_feat_rank.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed353d-626b-4f72-8066-1dc6ae13f538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
